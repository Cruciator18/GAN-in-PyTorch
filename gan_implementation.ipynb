{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzE_1CoC0T66",
        "outputId": "6e8161cf-6daf-46a5-a74e-77f5a830ab19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting generator.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile generator.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"\"\"\n",
        "    A standard Generator model for a Generative Adversarial Network (GAN).\n",
        "\n",
        "    The generator's job is to create realistic images from a random noise\n",
        "    vector (latent space). Its architecture uses transposed convolutions to\n",
        "    upsample the noise vector into an image-sized tensor.\n",
        "\n",
        "    Args:\n",
        "        noise_dim (int): The dimension of the input latent/noise vector.\n",
        "        image_channels (int): The number of channels for the output image.\n",
        "                              1 for grayscale, 3 for RGB.\n",
        "        hidden_dim (int): The feature dimension size for the intermediate layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, noise_dim, image_channels, hidden_dim=64):\n",
        "        super(Generator, self).__init__()\n",
        "        self.noise_dim = noise_dim\n",
        "        self.image_channels = image_channels\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # The neural network architecture is defined here.\n",
        "        self.model = nn.Sequential(\n",
        "            # Input: (batch_size, noise_dim, 1, 1)\n",
        "            # This block transforms the noise vector into a 4x4 feature map.\n",
        "            nn.ConvTranspose2d(self.noise_dim, self.hidden_dim * 8, kernel_size=4, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(self.hidden_dim * 8),\n",
        "            nn.ReLU(True),\n",
        "            # Output: (batch_size, hidden_dim * 8, 4, 4)\n",
        "\n",
        "            # This block upsamples to an 8x8 feature map.\n",
        "            nn.ConvTranspose2d(self.hidden_dim * 8, self.hidden_dim * 4, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(self.hidden_dim * 4),\n",
        "            nn.ReLU(True),\n",
        "            # Output: (batch_size, hidden_dim * 4, 8, 8)\n",
        "\n",
        "            # This block upsamples to a 16x16 feature map.\n",
        "            nn.ConvTranspose2d(self.hidden_dim * 4, self.hidden_dim * 2, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(self.hidden_dim * 2),\n",
        "            nn.ReLU(True),\n",
        "            # Output: (batch_size, hidden_dim * 2, 16, 16)\n",
        "\n",
        "            # This block upsamples to a 32x32 feature map.\n",
        "            nn.ConvTranspose2d(self.hidden_dim * 2, self.hidden_dim, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(self.hidden_dim),\n",
        "            nn.ReLU(True),\n",
        "            # Output: (batch_size, hidden_dim, 32, 32)\n",
        "\n",
        "            # This final block produces the 64x64 image.\n",
        "            nn.ConvTranspose2d(self.hidden_dim, self.image_channels, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            # The Tanh activation function is crucial here. It squashes the output\n",
        "            # pixel values to be in the range [-1, 1]. This matches the normalization\n",
        "            # we applied to the real images in the DataLoader.\n",
        "            nn.Tanh()\n",
        "            # Output: (batch_size, image_channels, 64, 64)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Defines the forward pass of the generator.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): The input noise tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: A tensor representing the generated image.\n",
        "        \"\"\"\n",
        "        return self.model(x)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # --- Example Usage ---\n",
        "    NOISE_DIM = 100\n",
        "    IMG_CHANNELS = 3\n",
        "    IMG_SIZE = 64\n",
        "    BATCH_SIZE = 128\n",
        "\n",
        "    # Instantiate the generator\n",
        "    generator = Generator(noise_dim=NOISE_DIM, image_channels=IMG_CHANNELS)\n",
        "    print(\"--- Generator Architecture ---\")\n",
        "    print(generator)\n",
        "\n",
        "    # Create a batch of random noise vectors.\n",
        "    # The noise is typically sampled from a standard normal distribution.\n",
        "    # The shape needs to be (batch_size, noise_dim, 1, 1) for the first\n",
        "    # ConvTranspose2d layer.\n",
        "    noise = torch.randn(BATCH_SIZE, NOISE_DIM, 1, 1)\n",
        "\n",
        "    # Pass the noise through the generator to create fake images\n",
        "    try:\n",
        "        fake_images = generator(noise)\n",
        "        print(\"\\n--- Model Forward Pass Successful ---\")\n",
        "        print(f\"Input noise shape: {noise.shape}\")\n",
        "        print(f\"Output image shape: {fake_images.shape}\") # Should be (BATCH_SIZE, IMG_CHANNELS, IMG_SIZE, IMG_SIZE)\n",
        "        print(f\"Output value range: Min={fake_images.min():.2f}, Max={fake_images.max():.2f}\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"\\n--- An error occurred ---\")\n",
        "        print(e)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    \"\"\"\n",
        "    A standard Discriminator model for a Generative Adversarial Network (GAN).\n",
        "\n",
        "    The discriminator's job is to distinguish between real images from a dataset\n",
        "    and fake images generated by the Generator. It's essentially a binary\n",
        "    classifier.\n",
        "\n",
        "    Args:\n",
        "        image_channels (int): The number of channels in the input image.\n",
        "                              1 for grayscale, 3 for RGB.\n",
        "        hidden_dim (int): The feature dimension size for the intermediate layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, image_channels, hidden_dim=64):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.image_channels = image_channels\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # The neural network architecture is defined here.\n",
        "        self.model = nn.Sequential(\n",
        "            # Input: (batch_size, image_channels, 64, 64)\n",
        "            nn.Conv2d(self.image_channels, self.hidden_dim, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # Output: (batch_size, hidden_dim, 32, 32)\n",
        "\n",
        "            nn.Conv2d(self.hidden_dim, self.hidden_dim * 2, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(self.hidden_dim * 2), # Correctly matches hidden_dim * 2 (128)\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # Output: (batch_size, hidden_dim * 2, 16, 16)\n",
        "\n",
        "            nn.Conv2d(self.hidden_dim * 2, self.hidden_dim * 4, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(self.hidden_dim * 4), # Correctly matches hidden_dim * 4 (256)\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # Output: (batch_size, hidden_dim * 4, 8, 8)\n",
        "\n",
        "            nn.Conv2d(self.hidden_dim * 4, self.hidden_dim * 8, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(self.hidden_dim * 8), # Correctly matches hidden_dim * 8 (512)\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # Output: (batch_size, hidden_dim * 8, 4, 4)\n",
        "\n",
        "            # Final classification layer to output a single probability score.\n",
        "            nn.Conv2d(self.hidden_dim * 8, 1, kernel_size=4, stride=1, padding=0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "            # Output: (batch_size, 1, 1, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Defines the forward pass of the discriminator.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): The input tensor (a batch of images).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: A tensor containing the probability score for each image.\n",
        "        \"\"\"\n",
        "        return self.model(x)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # --- Example Usage ---\n",
        "    IMG_CHANNELS = 3\n",
        "    IMG_SIZE = 64\n",
        "    BATCH_SIZE = 128\n",
        "\n",
        "    discriminator = Discriminator(image_channels=IMG_CHANNELS)\n",
        "    print(\"--- Discriminator Architecture ---\")\n",
        "    print(discriminator)\n",
        "\n",
        "    fake_images = torch.randn(BATCH_SIZE, IMG_CHANNELS, IMG_SIZE, IMG_SIZE)\n",
        "\n",
        "    # This line should now work without error\n",
        "    try:\n",
        "        output = discriminator(fake_images)\n",
        "        print(\"\\n--- Model Forward Pass Successful ---\")\n",
        "        print(f\"Input shape: {fake_images.shape}\")\n",
        "        print(f\"Output shape: {output.shape}\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"\\n--- An error occurred ---\")\n",
        "        print(e)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6i8CXLz1ceW",
        "outputId": "848b4586-4eaa-452e-a421-589bf53efa8e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Discriminator Architecture ---\n",
            "Discriminator(\n",
            "  (model): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (12): Sigmoid()\n",
            "  )\n",
            ")\n",
            "\n",
            "--- Model Forward Pass Successful ---\n",
            "Input shape: torch.Size([128, 3, 64, 64])\n",
            "Output shape: torch.Size([128, 1, 1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# --- Image Transformations for Grayscale NPY Data ---\n",
        "# The Quick, Draw! data is 28x28 grayscale. Our GAN expects 64x64 3-channel images.\n",
        "# This transform pipeline will handle the conversion.\n",
        "image_transforms = transforms.Compose([\n",
        "    # We first need to convert the NumPy array to a PIL Image, which the transforms expect.\n",
        "    # This is done inside the Dataset class.\n",
        "\n",
        "    transforms.Resize(64),          # Upsample the image from 28x28 to 64x64\n",
        "    transforms.CenterCrop(64),      # Ensure it's exactly 64x64\n",
        "\n",
        "    # This is a key step: The original data is grayscale (1 channel).\n",
        "    # Our GAN discriminator expects 3 channels (RGB).\n",
        "    # This transform converts the grayscale image to a 3-channel image by duplicating the channel.\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "\n",
        "    transforms.ToTensor(),          # Convert image to a PyTorch Tensor (values 0-1)\n",
        "\n",
        "    # Normalize the tensor to a range of [-1, 1] to match the generator's output\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "\n",
        "class NpyBitmapDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom PyTorch Dataset for loading images from a \"Quick, Draw!\" .npy file.\n",
        "\n",
        "    Args:\n",
        "        npy_path (str): The path to the .npy file.\n",
        "        transform (callable, optional): A function/transform to apply to each image.\n",
        "    \"\"\"\n",
        "    def __init__(self, npy_path, transform=None):\n",
        "        self.transform = transform\n",
        "\n",
        "        try:\n",
        "            # Load the entire dataset from the .npy file into memory.\n",
        "            # The data is expected to be uint8 (0-255).\n",
        "            self.data = np.load(npy_path)\n",
        "            print(f\"Successfully loaded {npy_path}.\")\n",
        "            print(f\"Dataset shape: {self.data.shape}\") # Should be (num_images, 784)\n",
        "            print(f\"Data type: {self.data.dtype}\")\n",
        "\n",
        "            # Add a check for the expected data size based on the potential reshape\n",
        "            expected_size = self.data.shape[0] * 28 * 28\n",
        "            print(f\"Expected size for reshape (num_images * 28 * 28): {expected_size}\")\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: The file was not found at {npy_path}\")\n",
        "            print(\"Please make sure the file exists and the path is correct.\")\n",
        "            self.data = np.array([]) # Create an empty array to avoid crashing\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while loading the file: {e}\")\n",
        "            self.data = np.array([])\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the total number of images in the dataset.\"\"\"\n",
        "        return self.data.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retrieves an image by its index, reshapes it, and applies transformations.\n",
        "        \"\"\"\n",
        "        # Get the flattened 784-pixel data for the given index\n",
        "        flat_image = self.data[idx]\n",
        "\n",
        "        # Reshape the data into a 28x28 pixel grayscale image\n",
        "        image_2d = flat_image.reshape(28, 28)\n",
        "\n",
        "        # Convert the NumPy array to a PIL Image.\n",
        "        # Transforms like Resize work best with PIL Images.\n",
        "        #\n",
        "        image = Image.fromarray(image_2d, mode='L') # 'L' mode is for grayscale\n",
        "\n",
        "        # Apply the transformations if they are defined\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # --- Example Usage ---\n",
        "    # This block demonstrates how to use the NpyBitmapDataset class.\n",
        "\n",
        "    # IMPORTANT: Update this path to where you have stored the .npy file.\n",
        "    NPY_FILE_PATH = r'/content/full_numpy_bitmap_camel.npy'\n",
        "\n",
        "    print(f\"--- Testing NpyBitmapDataset with file: {NPY_FILE_PATH} ---\")\n",
        "\n",
        "    if os.path.exists(NPY_FILE_PATH):\n",
        "        # Create an instance of the dataset\n",
        "        dataset = NpyBitmapDataset(npy_path=NPY_FILE_PATH, transform=image_transforms)\n",
        "\n",
        "        if len(dataset) > 0:\n",
        "            # Retrieve the first item from the dataset to test it\n",
        "            first_image = dataset[0]\n",
        "\n",
        "            print(\"\\n--- Dataset Test Successful ---\")\n",
        "            print(f\"Total number of images found: {len(dataset)}\")\n",
        "            print(f\"Shape of a single transformed image tensor: {first_image.shape}\") # Should be (3, 64, 64)\n",
        "            print(f\"Data type of the tensor: {first_image.dtype}\")\n",
        "            print(f\"Min value in the tensor: {first_image.min():.2f}\") # Should be ~ -1.0\n",
        "            print(f\"Max value in the tensor: {first_image.max():.2f}\") # Should be ~ 1.0\n",
        "        else:\n",
        "            print(\"\\nDataset was loaded but contains no data. Please check the .npy file.\")\n",
        "    else:\n",
        "        print(\"\\n--- Test Failed ---\")\n",
        "        print(f\"The file was not found. Please update the NPY_FILE_PATH variable in the script.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07yV-kVI14l1",
        "outputId": "8e47a079-d952-4cab-a6dd-bacad2986d3f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Testing NpyBitmapDataset with file: /content/full_numpy_bitmap_camel.npy ---\n",
            "An error occurred while loading the file: cannot reshape array of size 91226032 into shape (121399,784)\n",
            "\n",
            "Dataset was loaded but contains no data. Please check the .npy file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.utils as vutils\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "\n",
        "# Import the model and data loader classes we've created\n",
        "from generator import Generator\n",
        "from discriminator import Discriminator\n",
        "from npy_data_loader import NpyBitmapDataset, image_transforms\n",
        "\n",
        "\n",
        "# --- Hyperparameters ---\n",
        "# It's good practice to keep all hyperparameters in one place.\n",
        "LEARNING_RATE = 0.0002\n",
        "BETA1 = 0.5  # Recommended for Adam optimizer in DCGAN paper\n",
        "BATCH_SIZE = 128\n",
        "IMAGE_SIZE = 64\n",
        "IMAGE_CHANNELS = 3\n",
        "NOISE_DIM = 100\n",
        "NUM_EPOCHS = 25 # Increase this for better results\n",
        "HIDDEN_DIM = 64\n",
        "\n",
        "# --- Setup for Data and Directories ---\n",
        "print(\"--- Setting up environment ---\")\n",
        "\n",
        "# Create a dummy image directory for demonstration\n",
        "# In a real scenario, you would point this to your actual dataset folder.\n",
        "NPY_FILE = r'/content/full_numpy_bitmap_camel.npy'\n",
        "# Extract the directory path from the file path\n",
        "DATA_DIR = os.path.dirname(NPY_FILE)\n",
        "\n",
        "# Check if the data directory exists, and create it if it doesn't\n",
        "if not os.path.exists(DATA_DIR):\n",
        "    os.makedirs(DATA_DIR)\n",
        "    print(f\"Created data directory at '{DATA_DIR}'. Please place your .npy file here.\")\n",
        "\n",
        "# Create directories to save generated images and model checkpoints\n",
        "os.makedirs('results/real', exist_ok=True)\n",
        "os.makedirs('results/fake', exist_ok=True)\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "# Set the device (use GPU if available, otherwise CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "# --- Data Loading ---\n",
        "# Only attempt to load the dataset if the NPY file exists\n",
        "if os.path.exists(NPY_FILE):\n",
        "    dataset = NpyBitmapDataset(npy_path=NPY_FILE, transform=image_transforms)\n",
        "    # Only create DataLoader if the dataset is not empty\n",
        "    if len(dataset) > 0:\n",
        "        dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "        print(\"Data loader created.\")\n",
        "    else:\n",
        "        dataloader = None # Set dataloader to None if dataset is empty\n",
        "        print(\"Dataset is empty. Data loader was not created.\")\n",
        "else:\n",
        "    dataset = None # Set dataset to None if the file doesn't exist\n",
        "    dataloader = None # Set dataloader to None if the file doesn't exist\n",
        "    print(f\"Error: The file was not found at {NPY_FILE}. Data loader was not created.\")\n",
        "\n",
        "\n",
        "# --- Model Initialization ---\n",
        "# Create instances of the Generator and Discriminator\n",
        "generator = Generator(NOISE_DIM, IMAGE_CHANNELS, HIDDEN_DIM).to(device)\n",
        "discriminator = Discriminator(IMAGE_CHANNELS, HIDDEN_DIM).to(device)\n",
        "\n",
        "# Custom weight initialization as suggested in the DCGAN paper\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "generator.apply(weights_init)\n",
        "discriminator.apply(weights_init)\n",
        "print(\"Models created and weights initialized.\")\n",
        "\n",
        "# --- Optimizers and Loss Function ---\n",
        "# Binary Cross-Entropy loss is standard for GANs\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# We need separate optimizers for the generator and discriminator\n",
        "d_optimizer = optim.Adam(discriminator.parameters(), lr=LEARNING_RATE, betas=(BETA1, 0.999))\n",
        "g_optimizer = optim.Adam(generator.parameters(), lr=LEARNING_RATE, betas=(BETA1, 0.999))\n",
        "print(\"Optimizers and loss function defined.\")\n",
        "\n",
        "# Create a fixed noise vector to see how the generator improves over time\n",
        "fixed_noise = torch.randn(64, NOISE_DIM, 1, 1, device=device)\n",
        "\n",
        "# --- Training Loop ---\n",
        "print(\"\\n--- Starting Training Loop ---\")\n",
        "# Only start training if the dataloader was successfully created\n",
        "if dataloader is not None:\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        for i, real_images in enumerate(dataloader, 0):\n",
        "            # real_images is a batch of images from our dataset\n",
        "\n",
        "            ############################\n",
        "            # (1) Update Discriminator #\n",
        "            ############################\n",
        "\n",
        "            # --- Train with real images ---\n",
        "            discriminator.zero_grad()\n",
        "            real_images = real_images.to(device)\n",
        "            batch_size = real_images.size(0)\n",
        "\n",
        "            # Create labels for real images (all 1s)\n",
        "            real_labels = torch.ones(batch_size, device=device)\n",
        "\n",
        "            # Forward pass real batch through Discriminator\n",
        "            d_output_real = discriminator(real_images).view(-1)\n",
        "            # Calculate loss on all-real batch\n",
        "            d_loss_real = criterion(d_output_real, real_labels)\n",
        "            d_loss_real.backward()\n",
        "\n",
        "            # --- Train with fake images ---\n",
        "            # Generate a batch of noise vectors\n",
        "            noise = torch.randn(batch_size, NOISE_DIM, 1, 1, device=device)\n",
        "            # Generate fake images with the generator\n",
        "            fake_images = generator(noise)\n",
        "\n",
        "            # Create labels for fake images (all 0s)\n",
        "            fake_labels = torch.zeros(batch_size, device=device)\n",
        "\n",
        "            # Classify fake images with Discriminator.\n",
        "            # Use .detach() to avoid backpropagating through the Generator\n",
        "            d_output_fake = discriminator(fake_images.detach()).view(-1)\n",
        "            d_loss_fake = criterion(d_output_fake, fake_labels)\n",
        "            d_loss_fake.backward()\n",
        "\n",
        "            # Update Discriminator\n",
        "            d_optimizer.step()\n",
        "\n",
        "            # Total discriminator loss\n",
        "            d_loss = d_loss_real + d_loss_fake\n",
        "\n",
        "            ############################\n",
        "            # (2) Update Generator     #\n",
        "            ############################\n",
        "            generator.zero_grad()\n",
        "\n",
        "            # We need to re-classify the fake images with the updated discriminator\n",
        "            d_output_for_g = discriminator(fake_images).view(-1)\n",
        "\n",
        "            # Generator's goal is to make the discriminator think its images are real.\n",
        "            # So, we calculate its loss using real_labels (all 1s).\n",
        "            g_loss = criterion(d_output_for_g, real_labels)\n",
        "\n",
        "            # Calculate gradients for generator\n",
        "            g_loss.backward()\n",
        "\n",
        "            # Update Generator\n",
        "            g_optimizer.step()\n",
        "\n",
        "            # --- Logging and Visualization ---\n",
        "            if i % 50 == 0:\n",
        "                print(\n",
        "                    f'Epoch [{epoch+1}/{NUM_EPOCHS}] | Batch [{i}/{len(dataloader)}] | '\n",
        "                    f'D_loss: {d_loss.item():.4f} | G_loss: {g_loss.item():.4f}'\n",
        "                )\n",
        "\n",
        "        # After each epoch, save the generated images from the fixed_noise vector\n",
        "        with torch.no_grad():\n",
        "            fake_samples = generator(fixed_noise).detach().cpu()\n",
        "\n",
        "        # Save a grid of real images from the last batch\n",
        "        vutils.save_image(real_images, f\"results/real/epoch_{epoch+1}.png\", normalize=True)\n",
        "        # Save a grid of the generated fake images\n",
        "        vutils.save_image(fake_samples, f\"results/fake/epoch_{epoch+1}.png\", normalize=True)\n",
        "\n",
        "        # Save model checkpoints periodically\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            torch.save(generator.state_dict(), f'models/generator_epoch_{epoch+1}.pth')\n",
        "            torch.save(discriminator.state_dict(), f'models/discriminator_epoch_{epoch+1}.pth')\n",
        "            print(f\"Saved models at epoch {epoch+1}\")\n",
        "else:\n",
        "    print(\"\\nSkipping training as the data loader was not created.\")\n",
        "\n",
        "print(\"\\n--- Training Finished ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9I_bVTUb2O5C",
        "outputId": "6715ebf4-3ea5-4255-b63a-98f9f8b45380"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Setting up environment ---\n",
            "Using device: cuda\n",
            "Successfully loaded /content/full_numpy_bitmap_camel.npy.\n",
            "Dataset shape: (121399, 784)\n",
            "Data type: uint8\n",
            "Data loader created.\n",
            "Models created and weights initialized.\n",
            "Optimizers and loss function defined.\n",
            "\n",
            "--- Starting Training Loop ---\n",
            "Epoch [1/25] | Batch [0/949] | D_loss: 1.5534 | G_loss: 4.7227\n",
            "Epoch [1/25] | Batch [50/949] | D_loss: 0.0007 | G_loss: 15.4580\n",
            "Epoch [1/25] | Batch [100/949] | D_loss: 0.1310 | G_loss: 26.0599\n",
            "Epoch [1/25] | Batch [150/949] | D_loss: 0.1184 | G_loss: 15.1793\n",
            "Epoch [1/25] | Batch [200/949] | D_loss: 0.1708 | G_loss: 5.1316\n",
            "Epoch [1/25] | Batch [250/949] | D_loss: 0.0694 | G_loss: 10.6772\n",
            "Epoch [1/25] | Batch [300/949] | D_loss: 0.4936 | G_loss: 6.8403\n",
            "Epoch [1/25] | Batch [350/949] | D_loss: 0.3041 | G_loss: 2.8113\n",
            "Epoch [1/25] | Batch [400/949] | D_loss: 0.2260 | G_loss: 4.3807\n",
            "Epoch [1/25] | Batch [450/949] | D_loss: 0.1266 | G_loss: 4.2746\n",
            "Epoch [1/25] | Batch [500/949] | D_loss: 0.1067 | G_loss: 4.4455\n",
            "Epoch [1/25] | Batch [550/949] | D_loss: 0.2398 | G_loss: 3.6662\n",
            "Epoch [1/25] | Batch [600/949] | D_loss: 0.1169 | G_loss: 4.1521\n",
            "Epoch [1/25] | Batch [650/949] | D_loss: 0.1329 | G_loss: 3.9736\n",
            "Epoch [1/25] | Batch [700/949] | D_loss: 0.0962 | G_loss: 4.7864\n",
            "Epoch [1/25] | Batch [750/949] | D_loss: 0.3085 | G_loss: 3.8600\n",
            "Epoch [1/25] | Batch [800/949] | D_loss: 2.2624 | G_loss: 0.0757\n",
            "Epoch [1/25] | Batch [850/949] | D_loss: 0.1576 | G_loss: 3.5418\n",
            "Epoch [1/25] | Batch [900/949] | D_loss: 0.4435 | G_loss: 2.9944\n",
            "Epoch [2/25] | Batch [0/949] | D_loss: 0.2517 | G_loss: 3.2993\n",
            "Epoch [2/25] | Batch [50/949] | D_loss: 6.3677 | G_loss: 2.6128\n",
            "Epoch [2/25] | Batch [100/949] | D_loss: 0.2276 | G_loss: 3.0964\n",
            "Epoch [2/25] | Batch [150/949] | D_loss: 0.1764 | G_loss: 2.8036\n",
            "Epoch [2/25] | Batch [200/949] | D_loss: 0.1810 | G_loss: 2.6714\n",
            "Epoch [2/25] | Batch [250/949] | D_loss: 0.5280 | G_loss: 2.1237\n",
            "Epoch [2/25] | Batch [300/949] | D_loss: 0.3508 | G_loss: 1.7070\n",
            "Epoch [2/25] | Batch [350/949] | D_loss: 0.6864 | G_loss: 3.3777\n",
            "Epoch [2/25] | Batch [400/949] | D_loss: 0.2266 | G_loss: 2.7398\n",
            "Epoch [2/25] | Batch [450/949] | D_loss: 0.0800 | G_loss: 3.6457\n",
            "Epoch [2/25] | Batch [500/949] | D_loss: 1.4628 | G_loss: 6.9362\n",
            "Epoch [2/25] | Batch [550/949] | D_loss: 0.1328 | G_loss: 3.8074\n",
            "Epoch [2/25] | Batch [600/949] | D_loss: 0.5059 | G_loss: 3.8408\n",
            "Epoch [2/25] | Batch [650/949] | D_loss: 0.2005 | G_loss: 2.7668\n",
            "Epoch [2/25] | Batch [700/949] | D_loss: 0.1319 | G_loss: 2.9434\n",
            "Epoch [2/25] | Batch [750/949] | D_loss: 0.7780 | G_loss: 1.1375\n",
            "Epoch [2/25] | Batch [800/949] | D_loss: 0.2099 | G_loss: 3.9696\n",
            "Epoch [2/25] | Batch [850/949] | D_loss: 0.6620 | G_loss: 2.6556\n",
            "Epoch [2/25] | Batch [900/949] | D_loss: 0.1615 | G_loss: 3.2316\n",
            "Epoch [3/25] | Batch [0/949] | D_loss: 0.2256 | G_loss: 3.0595\n",
            "Epoch [3/25] | Batch [50/949] | D_loss: 0.1226 | G_loss: 3.7532\n",
            "Epoch [3/25] | Batch [100/949] | D_loss: 0.6585 | G_loss: 2.0184\n",
            "Epoch [3/25] | Batch [150/949] | D_loss: 0.5815 | G_loss: 4.6966\n",
            "Epoch [3/25] | Batch [200/949] | D_loss: 0.2800 | G_loss: 3.6223\n",
            "Epoch [3/25] | Batch [250/949] | D_loss: 0.5413 | G_loss: 4.2383\n",
            "Epoch [3/25] | Batch [300/949] | D_loss: 0.2008 | G_loss: 2.5375\n",
            "Epoch [3/25] | Batch [350/949] | D_loss: 0.1951 | G_loss: 2.9835\n",
            "Epoch [3/25] | Batch [400/949] | D_loss: 0.2209 | G_loss: 2.9029\n",
            "Epoch [3/25] | Batch [450/949] | D_loss: 0.0975 | G_loss: 3.2156\n",
            "Epoch [3/25] | Batch [500/949] | D_loss: 0.1402 | G_loss: 3.6991\n",
            "Epoch [3/25] | Batch [550/949] | D_loss: 0.0732 | G_loss: 4.1727\n",
            "Epoch [3/25] | Batch [600/949] | D_loss: 0.1996 | G_loss: 3.0173\n",
            "Epoch [3/25] | Batch [650/949] | D_loss: 0.0606 | G_loss: 4.2013\n",
            "Epoch [3/25] | Batch [700/949] | D_loss: 0.0826 | G_loss: 3.9297\n",
            "Epoch [3/25] | Batch [750/949] | D_loss: 0.3348 | G_loss: 2.9998\n",
            "Epoch [3/25] | Batch [800/949] | D_loss: 0.2699 | G_loss: 4.2531\n",
            "Epoch [3/25] | Batch [850/949] | D_loss: 0.2938 | G_loss: 2.7224\n",
            "Epoch [3/25] | Batch [900/949] | D_loss: 0.1033 | G_loss: 4.2717\n",
            "Epoch [4/25] | Batch [0/949] | D_loss: 0.0716 | G_loss: 4.2648\n",
            "Epoch [4/25] | Batch [50/949] | D_loss: 0.4802 | G_loss: 1.7543\n",
            "Epoch [4/25] | Batch [100/949] | D_loss: 0.1628 | G_loss: 2.6212\n",
            "Epoch [4/25] | Batch [150/949] | D_loss: 0.5005 | G_loss: 1.9552\n",
            "Epoch [4/25] | Batch [200/949] | D_loss: 0.3048 | G_loss: 2.8318\n",
            "Epoch [4/25] | Batch [250/949] | D_loss: 0.0610 | G_loss: 4.0249\n",
            "Epoch [4/25] | Batch [300/949] | D_loss: 0.0241 | G_loss: 5.1429\n",
            "Epoch [4/25] | Batch [350/949] | D_loss: 0.0675 | G_loss: 3.5219\n",
            "Epoch [4/25] | Batch [400/949] | D_loss: 0.0338 | G_loss: 5.0089\n",
            "Epoch [4/25] | Batch [450/949] | D_loss: 0.1842 | G_loss: 4.7035\n",
            "Epoch [4/25] | Batch [500/949] | D_loss: 1.1060 | G_loss: 1.1453\n",
            "Epoch [4/25] | Batch [550/949] | D_loss: 0.2091 | G_loss: 2.9624\n",
            "Epoch [4/25] | Batch [600/949] | D_loss: 0.1175 | G_loss: 4.1717\n",
            "Epoch [4/25] | Batch [650/949] | D_loss: 0.5435 | G_loss: 1.6550\n",
            "Epoch [4/25] | Batch [700/949] | D_loss: 0.0245 | G_loss: 5.1609\n",
            "Epoch [4/25] | Batch [750/949] | D_loss: 0.0961 | G_loss: 2.8810\n",
            "Epoch [4/25] | Batch [800/949] | D_loss: 0.0459 | G_loss: 4.7028\n",
            "Epoch [4/25] | Batch [850/949] | D_loss: 0.0531 | G_loss: 5.4067\n",
            "Epoch [4/25] | Batch [900/949] | D_loss: 0.4395 | G_loss: 2.3331\n",
            "Epoch [5/25] | Batch [0/949] | D_loss: 0.6898 | G_loss: 1.9319\n",
            "Epoch [5/25] | Batch [50/949] | D_loss: 1.5442 | G_loss: 0.0951\n",
            "Epoch [5/25] | Batch [100/949] | D_loss: 0.1361 | G_loss: 3.4784\n",
            "Epoch [5/25] | Batch [150/949] | D_loss: 0.2395 | G_loss: 3.0731\n",
            "Epoch [5/25] | Batch [200/949] | D_loss: 0.0520 | G_loss: 4.5087\n",
            "Epoch [5/25] | Batch [250/949] | D_loss: 0.6406 | G_loss: 12.1860\n",
            "Epoch [5/25] | Batch [300/949] | D_loss: 0.1411 | G_loss: 5.3369\n",
            "Epoch [5/25] | Batch [350/949] | D_loss: 0.5658 | G_loss: 2.7491\n",
            "Epoch [5/25] | Batch [400/949] | D_loss: 0.0837 | G_loss: 4.6066\n",
            "Epoch [5/25] | Batch [450/949] | D_loss: 0.0179 | G_loss: 5.0335\n",
            "Epoch [5/25] | Batch [500/949] | D_loss: 1.0330 | G_loss: 1.3272\n",
            "Epoch [5/25] | Batch [550/949] | D_loss: 0.6272 | G_loss: 1.5408\n",
            "Epoch [5/25] | Batch [600/949] | D_loss: 0.1987 | G_loss: 3.0678\n",
            "Epoch [5/25] | Batch [650/949] | D_loss: 0.1967 | G_loss: 2.6624\n",
            "Epoch [5/25] | Batch [700/949] | D_loss: 0.1028 | G_loss: 4.5961\n",
            "Epoch [5/25] | Batch [750/949] | D_loss: 0.5462 | G_loss: 3.1967\n",
            "Epoch [5/25] | Batch [800/949] | D_loss: 0.0703 | G_loss: 3.9613\n",
            "Epoch [5/25] | Batch [850/949] | D_loss: 0.1139 | G_loss: 4.0650\n",
            "Epoch [5/25] | Batch [900/949] | D_loss: 0.0778 | G_loss: 3.5243\n",
            "Saved models at epoch 5\n",
            "Epoch [6/25] | Batch [0/949] | D_loss: 0.0849 | G_loss: 3.0023\n",
            "Epoch [6/25] | Batch [50/949] | D_loss: 0.0321 | G_loss: 4.8630\n",
            "Epoch [6/25] | Batch [100/949] | D_loss: 0.0274 | G_loss: 5.1113\n",
            "Epoch [6/25] | Batch [150/949] | D_loss: 0.0294 | G_loss: 5.2190\n",
            "Epoch [6/25] | Batch [200/949] | D_loss: 0.0316 | G_loss: 5.0564\n",
            "Epoch [6/25] | Batch [250/949] | D_loss: 0.8060 | G_loss: 2.5165\n",
            "Epoch [6/25] | Batch [300/949] | D_loss: 0.2866 | G_loss: 2.6754\n",
            "Epoch [6/25] | Batch [350/949] | D_loss: 0.4420 | G_loss: 2.2498\n",
            "Epoch [6/25] | Batch [400/949] | D_loss: 0.0669 | G_loss: 4.3231\n",
            "Epoch [6/25] | Batch [450/949] | D_loss: 0.4643 | G_loss: 4.0885\n",
            "Epoch [6/25] | Batch [500/949] | D_loss: 0.9793 | G_loss: 0.6486\n",
            "Epoch [6/25] | Batch [550/949] | D_loss: 0.0445 | G_loss: 4.4011\n",
            "Epoch [6/25] | Batch [600/949] | D_loss: 0.2182 | G_loss: 2.8810\n",
            "Epoch [6/25] | Batch [650/949] | D_loss: 0.8454 | G_loss: 1.4214\n",
            "Epoch [6/25] | Batch [700/949] | D_loss: 0.8133 | G_loss: 1.5331\n",
            "Epoch [6/25] | Batch [750/949] | D_loss: 0.2774 | G_loss: 5.1037\n",
            "Epoch [6/25] | Batch [800/949] | D_loss: 0.0348 | G_loss: 4.2931\n",
            "Epoch [6/25] | Batch [850/949] | D_loss: 0.0314 | G_loss: 5.0481\n",
            "Epoch [6/25] | Batch [900/949] | D_loss: 0.0226 | G_loss: 5.4147\n",
            "Epoch [7/25] | Batch [0/949] | D_loss: 0.0392 | G_loss: 4.9640\n",
            "Epoch [7/25] | Batch [50/949] | D_loss: 0.0284 | G_loss: 5.1899\n",
            "Epoch [7/25] | Batch [100/949] | D_loss: 0.0190 | G_loss: 5.8493\n",
            "Epoch [7/25] | Batch [150/949] | D_loss: 0.5439 | G_loss: 3.1861\n",
            "Epoch [7/25] | Batch [200/949] | D_loss: 0.2319 | G_loss: 2.7168\n",
            "Epoch [7/25] | Batch [250/949] | D_loss: 0.5528 | G_loss: 6.0890\n",
            "Epoch [7/25] | Batch [300/949] | D_loss: 0.5769 | G_loss: 1.6886\n",
            "Epoch [7/25] | Batch [350/949] | D_loss: 0.2400 | G_loss: 3.3722\n",
            "Epoch [7/25] | Batch [400/949] | D_loss: 0.1000 | G_loss: 4.1214\n",
            "Epoch [7/25] | Batch [450/949] | D_loss: 0.0526 | G_loss: 4.2365\n",
            "Epoch [7/25] | Batch [500/949] | D_loss: 0.0475 | G_loss: 5.4193\n",
            "Epoch [7/25] | Batch [550/949] | D_loss: 0.0251 | G_loss: 5.2421\n",
            "Epoch [7/25] | Batch [600/949] | D_loss: 2.1172 | G_loss: 2.0548\n",
            "Epoch [7/25] | Batch [650/949] | D_loss: 0.5717 | G_loss: 1.9999\n",
            "Epoch [7/25] | Batch [700/949] | D_loss: 0.2782 | G_loss: 2.5496\n",
            "Epoch [7/25] | Batch [750/949] | D_loss: 0.6046 | G_loss: 1.8273\n",
            "Epoch [7/25] | Batch [800/949] | D_loss: 0.1241 | G_loss: 3.5938\n",
            "Epoch [7/25] | Batch [850/949] | D_loss: 0.4986 | G_loss: 1.8575\n",
            "Epoch [7/25] | Batch [900/949] | D_loss: 0.3043 | G_loss: 2.8688\n",
            "Epoch [8/25] | Batch [0/949] | D_loss: 0.1296 | G_loss: 3.2948\n",
            "Epoch [8/25] | Batch [50/949] | D_loss: 0.0677 | G_loss: 3.7804\n",
            "Epoch [8/25] | Batch [100/949] | D_loss: 0.0307 | G_loss: 4.7401\n",
            "Epoch [8/25] | Batch [150/949] | D_loss: 0.0223 | G_loss: 4.9825\n",
            "Epoch [8/25] | Batch [200/949] | D_loss: 1.5606 | G_loss: 2.1400\n",
            "Epoch [8/25] | Batch [250/949] | D_loss: 1.0159 | G_loss: 1.4956\n",
            "Epoch [8/25] | Batch [300/949] | D_loss: 0.9019 | G_loss: 2.9721\n",
            "Epoch [8/25] | Batch [350/949] | D_loss: 0.6298 | G_loss: 6.0562\n",
            "Epoch [8/25] | Batch [400/949] | D_loss: 0.6321 | G_loss: 6.9555\n",
            "Epoch [8/25] | Batch [450/949] | D_loss: 0.6420 | G_loss: 1.6801\n",
            "Epoch [8/25] | Batch [500/949] | D_loss: 0.1778 | G_loss: 3.2749\n",
            "Epoch [8/25] | Batch [550/949] | D_loss: 0.0936 | G_loss: 4.5106\n",
            "Epoch [8/25] | Batch [600/949] | D_loss: 0.1250 | G_loss: 6.3074\n",
            "Epoch [8/25] | Batch [650/949] | D_loss: 0.5782 | G_loss: 2.2703\n",
            "Epoch [8/25] | Batch [700/949] | D_loss: 0.1623 | G_loss: 3.1265\n",
            "Epoch [8/25] | Batch [750/949] | D_loss: 0.0553 | G_loss: 4.3840\n",
            "Epoch [8/25] | Batch [800/949] | D_loss: 0.0478 | G_loss: 4.9117\n",
            "Epoch [8/25] | Batch [850/949] | D_loss: 1.7018 | G_loss: 0.9110\n",
            "Epoch [8/25] | Batch [900/949] | D_loss: 0.3383 | G_loss: 2.8701\n",
            "Epoch [9/25] | Batch [0/949] | D_loss: 0.6927 | G_loss: 7.2741\n",
            "Epoch [9/25] | Batch [50/949] | D_loss: 0.0960 | G_loss: 3.8412\n",
            "Epoch [9/25] | Batch [100/949] | D_loss: 0.7753 | G_loss: 1.8358\n",
            "Epoch [9/25] | Batch [150/949] | D_loss: 0.4696 | G_loss: 1.8147\n",
            "Epoch [9/25] | Batch [200/949] | D_loss: 0.2021 | G_loss: 3.9016\n",
            "Epoch [9/25] | Batch [250/949] | D_loss: 0.3769 | G_loss: 5.2666\n",
            "Epoch [9/25] | Batch [300/949] | D_loss: 0.3572 | G_loss: 2.7169\n",
            "Epoch [9/25] | Batch [350/949] | D_loss: 0.0895 | G_loss: 3.7124\n",
            "Epoch [9/25] | Batch [400/949] | D_loss: 0.0501 | G_loss: 4.3501\n",
            "Epoch [9/25] | Batch [450/949] | D_loss: 0.0418 | G_loss: 4.7136\n",
            "Epoch [9/25] | Batch [500/949] | D_loss: 0.0379 | G_loss: 5.0579\n",
            "Epoch [9/25] | Batch [550/949] | D_loss: 0.0183 | G_loss: 4.8400\n",
            "Epoch [9/25] | Batch [600/949] | D_loss: 0.0414 | G_loss: 6.1616\n",
            "Epoch [9/25] | Batch [650/949] | D_loss: 0.0136 | G_loss: 6.1759\n",
            "Epoch [9/25] | Batch [700/949] | D_loss: 0.0131 | G_loss: 5.3858\n",
            "Epoch [9/25] | Batch [750/949] | D_loss: 0.0244 | G_loss: 5.3678\n",
            "Epoch [9/25] | Batch [800/949] | D_loss: 0.1033 | G_loss: 9.5814\n",
            "Epoch [9/25] | Batch [850/949] | D_loss: 0.2807 | G_loss: 2.8706\n",
            "Epoch [9/25] | Batch [900/949] | D_loss: 0.2426 | G_loss: 4.3336\n",
            "Epoch [10/25] | Batch [0/949] | D_loss: 0.5216 | G_loss: 1.9743\n",
            "Epoch [10/25] | Batch [50/949] | D_loss: 0.5622 | G_loss: 2.1849\n",
            "Epoch [10/25] | Batch [100/949] | D_loss: 0.1923 | G_loss: 6.3871\n",
            "Epoch [10/25] | Batch [150/949] | D_loss: 1.5160 | G_loss: 1.4084\n",
            "Epoch [10/25] | Batch [200/949] | D_loss: 1.7941 | G_loss: 1.8136\n",
            "Epoch [10/25] | Batch [250/949] | D_loss: 0.0398 | G_loss: 5.0002\n",
            "Epoch [10/25] | Batch [300/949] | D_loss: 0.0375 | G_loss: 5.1803\n",
            "Epoch [10/25] | Batch [350/949] | D_loss: 0.0137 | G_loss: 5.6052\n",
            "Epoch [10/25] | Batch [400/949] | D_loss: 0.0320 | G_loss: 5.3946\n",
            "Epoch [10/25] | Batch [450/949] | D_loss: 0.6538 | G_loss: 2.1334\n",
            "Epoch [10/25] | Batch [500/949] | D_loss: 0.2503 | G_loss: 3.7458\n",
            "Epoch [10/25] | Batch [550/949] | D_loss: 0.4362 | G_loss: 4.0209\n",
            "Epoch [10/25] | Batch [600/949] | D_loss: 0.0290 | G_loss: 6.6591\n",
            "Epoch [10/25] | Batch [650/949] | D_loss: 0.8421 | G_loss: 3.4702\n",
            "Epoch [10/25] | Batch [700/949] | D_loss: 0.1765 | G_loss: 2.3196\n",
            "Epoch [10/25] | Batch [750/949] | D_loss: 0.0332 | G_loss: 4.9533\n",
            "Epoch [10/25] | Batch [800/949] | D_loss: 0.0525 | G_loss: 4.4347\n",
            "Epoch [10/25] | Batch [850/949] | D_loss: 0.0275 | G_loss: 4.9052\n",
            "Epoch [10/25] | Batch [900/949] | D_loss: 0.0128 | G_loss: 5.6454\n",
            "Saved models at epoch 10\n",
            "Epoch [11/25] | Batch [0/949] | D_loss: 0.0079 | G_loss: 6.3422\n",
            "Epoch [11/25] | Batch [50/949] | D_loss: 0.0702 | G_loss: 4.6384\n",
            "Epoch [11/25] | Batch [100/949] | D_loss: 0.3219 | G_loss: 2.5350\n",
            "Epoch [11/25] | Batch [150/949] | D_loss: 0.2370 | G_loss: 4.8593\n",
            "Epoch [11/25] | Batch [200/949] | D_loss: 0.0878 | G_loss: 3.5921\n",
            "Epoch [11/25] | Batch [250/949] | D_loss: 0.4413 | G_loss: 2.6605\n",
            "Epoch [11/25] | Batch [300/949] | D_loss: 0.0349 | G_loss: 5.6090\n",
            "Epoch [11/25] | Batch [350/949] | D_loss: 0.0339 | G_loss: 5.2450\n",
            "Epoch [11/25] | Batch [400/949] | D_loss: 0.0247 | G_loss: 5.1225\n",
            "Epoch [11/25] | Batch [450/949] | D_loss: 0.0192 | G_loss: 4.8882\n",
            "Epoch [11/25] | Batch [500/949] | D_loss: 0.3918 | G_loss: 3.2396\n",
            "Epoch [11/25] | Batch [550/949] | D_loss: 0.5868 | G_loss: 0.7474\n",
            "Epoch [11/25] | Batch [600/949] | D_loss: 0.0613 | G_loss: 4.2280\n",
            "Epoch [11/25] | Batch [650/949] | D_loss: 0.0249 | G_loss: 4.9357\n",
            "Epoch [11/25] | Batch [700/949] | D_loss: 0.0126 | G_loss: 6.0029\n",
            "Epoch [11/25] | Batch [750/949] | D_loss: 0.0396 | G_loss: 4.2722\n",
            "Epoch [11/25] | Batch [800/949] | D_loss: 0.0159 | G_loss: 5.7252\n",
            "Epoch [11/25] | Batch [850/949] | D_loss: 0.0053 | G_loss: 6.6561\n",
            "Epoch [11/25] | Batch [900/949] | D_loss: 0.0092 | G_loss: 6.0436\n",
            "Epoch [12/25] | Batch [0/949] | D_loss: 0.0036 | G_loss: 7.8582\n",
            "Epoch [12/25] | Batch [50/949] | D_loss: 0.6895 | G_loss: 13.4797\n",
            "Epoch [12/25] | Batch [100/949] | D_loss: 0.2942 | G_loss: 3.5869\n",
            "Epoch [12/25] | Batch [150/949] | D_loss: 0.2205 | G_loss: 4.5468\n",
            "Epoch [12/25] | Batch [200/949] | D_loss: 0.1215 | G_loss: 4.2031\n",
            "Epoch [12/25] | Batch [250/949] | D_loss: 0.1073 | G_loss: 3.8107\n",
            "Epoch [12/25] | Batch [300/949] | D_loss: 1.0325 | G_loss: 1.7756\n",
            "Epoch [12/25] | Batch [350/949] | D_loss: 0.3975 | G_loss: 2.6774\n",
            "Epoch [12/25] | Batch [400/949] | D_loss: 0.0699 | G_loss: 4.1518\n",
            "Epoch [12/25] | Batch [450/949] | D_loss: 0.8970 | G_loss: 1.2237\n",
            "Epoch [12/25] | Batch [500/949] | D_loss: 0.3973 | G_loss: 2.2274\n",
            "Epoch [12/25] | Batch [550/949] | D_loss: 0.1780 | G_loss: 4.2810\n",
            "Epoch [12/25] | Batch [600/949] | D_loss: 0.0782 | G_loss: 4.3661\n",
            "Epoch [12/25] | Batch [650/949] | D_loss: 0.0496 | G_loss: 4.4834\n",
            "Epoch [12/25] | Batch [700/949] | D_loss: 0.0130 | G_loss: 5.9469\n",
            "Epoch [12/25] | Batch [750/949] | D_loss: 0.7772 | G_loss: 1.0593\n",
            "Epoch [12/25] | Batch [800/949] | D_loss: 0.4657 | G_loss: 1.8082\n",
            "Epoch [12/25] | Batch [850/949] | D_loss: 0.1925 | G_loss: 3.2446\n",
            "Epoch [12/25] | Batch [900/949] | D_loss: 0.1040 | G_loss: 3.7524\n",
            "Epoch [13/25] | Batch [0/949] | D_loss: 0.0280 | G_loss: 4.7160\n",
            "Epoch [13/25] | Batch [50/949] | D_loss: 0.0276 | G_loss: 4.9794\n",
            "Epoch [13/25] | Batch [100/949] | D_loss: 0.0177 | G_loss: 5.5023\n",
            "Epoch [13/25] | Batch [150/949] | D_loss: 0.6564 | G_loss: 1.7565\n",
            "Epoch [13/25] | Batch [200/949] | D_loss: 0.3989 | G_loss: 2.3807\n",
            "Epoch [13/25] | Batch [250/949] | D_loss: 0.2398 | G_loss: 2.4093\n",
            "Epoch [13/25] | Batch [300/949] | D_loss: 0.7639 | G_loss: 2.1625\n",
            "Epoch [13/25] | Batch [350/949] | D_loss: 0.2165 | G_loss: 2.7306\n",
            "Epoch [13/25] | Batch [400/949] | D_loss: 1.0777 | G_loss: 1.0597\n",
            "Epoch [13/25] | Batch [450/949] | D_loss: 0.1334 | G_loss: 3.6294\n",
            "Epoch [13/25] | Batch [500/949] | D_loss: 0.1569 | G_loss: 3.4526\n",
            "Epoch [13/25] | Batch [550/949] | D_loss: 0.0632 | G_loss: 4.1735\n",
            "Epoch [13/25] | Batch [600/949] | D_loss: 0.0361 | G_loss: 4.6084\n",
            "Epoch [13/25] | Batch [650/949] | D_loss: 0.0226 | G_loss: 5.2730\n",
            "Epoch [13/25] | Batch [700/949] | D_loss: 0.0122 | G_loss: 5.8188\n",
            "Epoch [13/25] | Batch [750/949] | D_loss: 0.0450 | G_loss: 6.5195\n",
            "Epoch [13/25] | Batch [800/949] | D_loss: 0.5533 | G_loss: 1.7520\n",
            "Epoch [13/25] | Batch [850/949] | D_loss: 0.1416 | G_loss: 4.5381\n",
            "Epoch [13/25] | Batch [900/949] | D_loss: 0.0428 | G_loss: 4.5167\n",
            "Epoch [14/25] | Batch [0/949] | D_loss: 0.0154 | G_loss: 5.3188\n",
            "Epoch [14/25] | Batch [50/949] | D_loss: 0.0121 | G_loss: 5.9881\n",
            "Epoch [14/25] | Batch [100/949] | D_loss: 0.0095 | G_loss: 5.8948\n",
            "Epoch [14/25] | Batch [150/949] | D_loss: 0.0060 | G_loss: 6.5602\n",
            "Epoch [14/25] | Batch [200/949] | D_loss: 0.0101 | G_loss: 6.1356\n",
            "Epoch [14/25] | Batch [250/949] | D_loss: 0.0037 | G_loss: 7.1970\n",
            "Epoch [14/25] | Batch [300/949] | D_loss: 0.0055 | G_loss: 6.5126\n",
            "Epoch [14/25] | Batch [350/949] | D_loss: 0.0062 | G_loss: 7.5075\n",
            "Epoch [14/25] | Batch [400/949] | D_loss: 0.0056 | G_loss: 6.6021\n",
            "Epoch [14/25] | Batch [450/949] | D_loss: 0.0035 | G_loss: 8.5168\n",
            "Epoch [14/25] | Batch [500/949] | D_loss: 0.0070 | G_loss: 6.4472\n",
            "Epoch [14/25] | Batch [550/949] | D_loss: 0.0138 | G_loss: 6.0799\n",
            "Epoch [14/25] | Batch [600/949] | D_loss: 1.3533 | G_loss: 0.3167\n",
            "Epoch [14/25] | Batch [650/949] | D_loss: 0.3494 | G_loss: 3.5806\n",
            "Epoch [14/25] | Batch [700/949] | D_loss: 0.7957 | G_loss: 6.8762\n",
            "Epoch [14/25] | Batch [750/949] | D_loss: 0.0917 | G_loss: 4.0162\n",
            "Epoch [14/25] | Batch [800/949] | D_loss: 0.1374 | G_loss: 4.3237\n",
            "Epoch [14/25] | Batch [850/949] | D_loss: 0.5541 | G_loss: 1.7175\n",
            "Epoch [14/25] | Batch [900/949] | D_loss: 0.0676 | G_loss: 3.8536\n",
            "Epoch [15/25] | Batch [0/949] | D_loss: 0.3291 | G_loss: 2.5009\n",
            "Epoch [15/25] | Batch [50/949] | D_loss: 0.0636 | G_loss: 4.4610\n",
            "Epoch [15/25] | Batch [100/949] | D_loss: 0.1561 | G_loss: 3.6994\n",
            "Epoch [15/25] | Batch [150/949] | D_loss: 0.7154 | G_loss: 1.7959\n",
            "Epoch [15/25] | Batch [200/949] | D_loss: 0.2434 | G_loss: 3.7641\n",
            "Epoch [15/25] | Batch [250/949] | D_loss: 0.0597 | G_loss: 4.5797\n",
            "Epoch [15/25] | Batch [300/949] | D_loss: 0.0454 | G_loss: 4.8958\n",
            "Epoch [15/25] | Batch [350/949] | D_loss: 0.0328 | G_loss: 5.7115\n",
            "Epoch [15/25] | Batch [400/949] | D_loss: 0.0038 | G_loss: 7.0408\n",
            "Epoch [15/25] | Batch [450/949] | D_loss: 0.7932 | G_loss: 3.0932\n",
            "Epoch [15/25] | Batch [500/949] | D_loss: 0.1530 | G_loss: 2.6247\n",
            "Epoch [15/25] | Batch [550/949] | D_loss: 0.2279 | G_loss: 3.0296\n",
            "Epoch [15/25] | Batch [600/949] | D_loss: 0.1392 | G_loss: 4.0697\n",
            "Epoch [15/25] | Batch [650/949] | D_loss: 0.1962 | G_loss: 10.6604\n",
            "Epoch [15/25] | Batch [700/949] | D_loss: 0.5986 | G_loss: 2.0995\n",
            "Epoch [15/25] | Batch [750/949] | D_loss: 0.2390 | G_loss: 3.3413\n",
            "Epoch [15/25] | Batch [800/949] | D_loss: 0.3615 | G_loss: 3.7955\n",
            "Epoch [15/25] | Batch [850/949] | D_loss: 0.0655 | G_loss: 4.6304\n",
            "Epoch [15/25] | Batch [900/949] | D_loss: 0.0323 | G_loss: 5.0585\n",
            "Saved models at epoch 15\n",
            "Epoch [16/25] | Batch [0/949] | D_loss: 0.0174 | G_loss: 5.8908\n",
            "Epoch [16/25] | Batch [50/949] | D_loss: 0.3643 | G_loss: 2.0108\n",
            "Epoch [16/25] | Batch [100/949] | D_loss: 0.1372 | G_loss: 3.7570\n",
            "Epoch [16/25] | Batch [150/949] | D_loss: 0.1821 | G_loss: 3.4980\n",
            "Epoch [16/25] | Batch [200/949] | D_loss: 0.0381 | G_loss: 4.3612\n",
            "Epoch [16/25] | Batch [250/949] | D_loss: 0.0212 | G_loss: 5.4086\n",
            "Epoch [16/25] | Batch [300/949] | D_loss: 0.0115 | G_loss: 5.7723\n",
            "Epoch [16/25] | Batch [350/949] | D_loss: 0.0101 | G_loss: 5.9986\n",
            "Epoch [16/25] | Batch [400/949] | D_loss: 0.0069 | G_loss: 5.9103\n",
            "Epoch [16/25] | Batch [450/949] | D_loss: 0.0075 | G_loss: 5.7762\n",
            "Epoch [16/25] | Batch [500/949] | D_loss: 0.0072 | G_loss: 6.1947\n",
            "Epoch [16/25] | Batch [550/949] | D_loss: 0.0137 | G_loss: 5.4893\n",
            "Epoch [16/25] | Batch [600/949] | D_loss: 0.0133 | G_loss: 6.4017\n",
            "Epoch [16/25] | Batch [650/949] | D_loss: 0.0154 | G_loss: 7.1630\n",
            "Epoch [16/25] | Batch [700/949] | D_loss: 1.2645 | G_loss: 0.5786\n",
            "Epoch [16/25] | Batch [750/949] | D_loss: 0.1407 | G_loss: 3.5791\n",
            "Epoch [16/25] | Batch [800/949] | D_loss: 0.1114 | G_loss: 3.7978\n",
            "Epoch [16/25] | Batch [850/949] | D_loss: 0.0600 | G_loss: 4.1671\n",
            "Epoch [16/25] | Batch [900/949] | D_loss: 0.0242 | G_loss: 5.7122\n",
            "Epoch [17/25] | Batch [0/949] | D_loss: 0.0144 | G_loss: 6.4359\n",
            "Epoch [17/25] | Batch [50/949] | D_loss: 0.5358 | G_loss: 2.7896\n",
            "Epoch [17/25] | Batch [100/949] | D_loss: 0.1300 | G_loss: 3.4859\n",
            "Epoch [17/25] | Batch [150/949] | D_loss: 0.0577 | G_loss: 4.3859\n",
            "Epoch [17/25] | Batch [200/949] | D_loss: 0.2336 | G_loss: 5.3497\n",
            "Epoch [17/25] | Batch [250/949] | D_loss: 0.2125 | G_loss: 3.6660\n",
            "Epoch [17/25] | Batch [300/949] | D_loss: 0.0797 | G_loss: 4.8103\n",
            "Epoch [17/25] | Batch [350/949] | D_loss: 0.0713 | G_loss: 4.6212\n",
            "Epoch [17/25] | Batch [400/949] | D_loss: 0.0405 | G_loss: 5.2127\n",
            "Epoch [17/25] | Batch [450/949] | D_loss: 0.0217 | G_loss: 5.1750\n",
            "Epoch [17/25] | Batch [500/949] | D_loss: 0.0114 | G_loss: 5.9708\n",
            "Epoch [17/25] | Batch [550/949] | D_loss: 0.0098 | G_loss: 6.2158\n",
            "Epoch [17/25] | Batch [600/949] | D_loss: 0.0117 | G_loss: 6.0048\n",
            "Epoch [17/25] | Batch [650/949] | D_loss: 0.0120 | G_loss: 5.5273\n",
            "Epoch [17/25] | Batch [700/949] | D_loss: 0.0071 | G_loss: 6.6811\n",
            "Epoch [17/25] | Batch [750/949] | D_loss: 0.0105 | G_loss: 5.8760\n",
            "Epoch [17/25] | Batch [800/949] | D_loss: 0.0094 | G_loss: 6.1486\n",
            "Epoch [17/25] | Batch [850/949] | D_loss: 0.0049 | G_loss: 6.4891\n",
            "Epoch [17/25] | Batch [900/949] | D_loss: 0.0039 | G_loss: 6.7950\n",
            "Epoch [18/25] | Batch [0/949] | D_loss: 0.0040 | G_loss: 6.3853\n",
            "Epoch [18/25] | Batch [50/949] | D_loss: 0.0058 | G_loss: 6.3753\n",
            "Epoch [18/25] | Batch [100/949] | D_loss: 0.0041 | G_loss: 7.0340\n",
            "Epoch [18/25] | Batch [150/949] | D_loss: 0.0036 | G_loss: 7.7498\n",
            "Epoch [18/25] | Batch [200/949] | D_loss: 0.0049 | G_loss: 6.5245\n",
            "Epoch [18/25] | Batch [250/949] | D_loss: 0.0032 | G_loss: 7.1454\n",
            "Epoch [18/25] | Batch [300/949] | D_loss: 0.0056 | G_loss: 6.8799\n",
            "Epoch [18/25] | Batch [350/949] | D_loss: 0.0050 | G_loss: 7.0056\n",
            "Epoch [18/25] | Batch [400/949] | D_loss: 0.0040 | G_loss: 7.6438\n",
            "Epoch [18/25] | Batch [450/949] | D_loss: 0.0036 | G_loss: 7.2228\n",
            "Epoch [18/25] | Batch [500/949] | D_loss: 2.1657 | G_loss: 4.0274\n",
            "Epoch [18/25] | Batch [550/949] | D_loss: 0.3143 | G_loss: 2.3684\n",
            "Epoch [18/25] | Batch [600/949] | D_loss: 0.8892 | G_loss: 1.3480\n",
            "Epoch [18/25] | Batch [650/949] | D_loss: 0.2244 | G_loss: 5.6006\n",
            "Epoch [18/25] | Batch [700/949] | D_loss: 2.3645 | G_loss: 4.8234\n",
            "Epoch [18/25] | Batch [750/949] | D_loss: 0.0965 | G_loss: 4.2950\n",
            "Epoch [18/25] | Batch [800/949] | D_loss: 0.0841 | G_loss: 5.9250\n",
            "Epoch [18/25] | Batch [850/949] | D_loss: 0.0252 | G_loss: 5.6520\n",
            "Epoch [18/25] | Batch [900/949] | D_loss: 0.0196 | G_loss: 5.9977\n",
            "Epoch [19/25] | Batch [0/949] | D_loss: 0.0063 | G_loss: 6.5505\n",
            "Epoch [19/25] | Batch [50/949] | D_loss: 0.0070 | G_loss: 6.3746\n",
            "Epoch [19/25] | Batch [100/949] | D_loss: 0.0028 | G_loss: 6.7079\n",
            "Epoch [19/25] | Batch [150/949] | D_loss: 0.0037 | G_loss: 7.5822\n",
            "Epoch [19/25] | Batch [200/949] | D_loss: 0.0046 | G_loss: 6.9030\n",
            "Epoch [19/25] | Batch [250/949] | D_loss: 0.5573 | G_loss: 1.4675\n",
            "Epoch [19/25] | Batch [300/949] | D_loss: 0.9528 | G_loss: 2.4516\n",
            "Epoch [19/25] | Batch [350/949] | D_loss: 0.0722 | G_loss: 4.0805\n",
            "Epoch [19/25] | Batch [400/949] | D_loss: 0.0305 | G_loss: 6.0026\n",
            "Epoch [19/25] | Batch [450/949] | D_loss: 0.0080 | G_loss: 6.3665\n",
            "Epoch [19/25] | Batch [500/949] | D_loss: 0.9151 | G_loss: 1.6328\n",
            "Epoch [19/25] | Batch [550/949] | D_loss: 0.4002 | G_loss: 1.9696\n",
            "Epoch [19/25] | Batch [600/949] | D_loss: 0.3071 | G_loss: 2.6723\n",
            "Epoch [19/25] | Batch [650/949] | D_loss: 1.5048 | G_loss: 0.5164\n",
            "Epoch [19/25] | Batch [700/949] | D_loss: 0.1180 | G_loss: 4.5805\n",
            "Epoch [19/25] | Batch [750/949] | D_loss: 0.1444 | G_loss: 3.5312\n",
            "Epoch [19/25] | Batch [800/949] | D_loss: 0.5515 | G_loss: 3.2041\n",
            "Epoch [19/25] | Batch [850/949] | D_loss: 0.0881 | G_loss: 4.2982\n",
            "Epoch [19/25] | Batch [900/949] | D_loss: 0.0283 | G_loss: 5.3981\n",
            "Epoch [20/25] | Batch [0/949] | D_loss: 0.3554 | G_loss: 2.0835\n",
            "Epoch [20/25] | Batch [50/949] | D_loss: 0.1188 | G_loss: 3.0565\n",
            "Epoch [20/25] | Batch [100/949] | D_loss: 0.1177 | G_loss: 3.6189\n",
            "Epoch [20/25] | Batch [150/949] | D_loss: 0.0107 | G_loss: 5.8468\n",
            "Epoch [20/25] | Batch [200/949] | D_loss: 0.0094 | G_loss: 5.9748\n",
            "Epoch [20/25] | Batch [250/949] | D_loss: 0.0080 | G_loss: 6.0531\n",
            "Epoch [20/25] | Batch [300/949] | D_loss: 0.0114 | G_loss: 6.1246\n",
            "Epoch [20/25] | Batch [350/949] | D_loss: 0.0098 | G_loss: 6.2135\n",
            "Epoch [20/25] | Batch [400/949] | D_loss: 0.0064 | G_loss: 6.2701\n",
            "Epoch [20/25] | Batch [450/949] | D_loss: 0.0089 | G_loss: 5.8503\n",
            "Epoch [20/25] | Batch [500/949] | D_loss: 0.1116 | G_loss: 3.6480\n",
            "Epoch [20/25] | Batch [550/949] | D_loss: 0.0315 | G_loss: 4.5914\n",
            "Epoch [20/25] | Batch [600/949] | D_loss: 0.0250 | G_loss: 5.8097\n",
            "Epoch [20/25] | Batch [650/949] | D_loss: 0.0162 | G_loss: 6.7613\n",
            "Epoch [20/25] | Batch [700/949] | D_loss: 0.0095 | G_loss: 6.0597\n",
            "Epoch [20/25] | Batch [750/949] | D_loss: 0.0065 | G_loss: 6.0528\n",
            "Epoch [20/25] | Batch [800/949] | D_loss: 0.0067 | G_loss: 6.1969\n",
            "Epoch [20/25] | Batch [850/949] | D_loss: 0.0034 | G_loss: 7.4072\n",
            "Epoch [20/25] | Batch [900/949] | D_loss: 0.0057 | G_loss: 6.3420\n",
            "Saved models at epoch 20\n",
            "Epoch [21/25] | Batch [0/949] | D_loss: 0.0025 | G_loss: 7.2512\n",
            "Epoch [21/25] | Batch [50/949] | D_loss: 0.0044 | G_loss: 6.0323\n",
            "Epoch [21/25] | Batch [100/949] | D_loss: 0.0029 | G_loss: 7.1655\n",
            "Epoch [21/25] | Batch [150/949] | D_loss: 0.0041 | G_loss: 6.6462\n",
            "Epoch [21/25] | Batch [200/949] | D_loss: 0.0052 | G_loss: 6.9077\n",
            "Epoch [21/25] | Batch [250/949] | D_loss: 0.0035 | G_loss: 7.5441\n",
            "Epoch [21/25] | Batch [300/949] | D_loss: 0.5389 | G_loss: 2.2872\n",
            "Epoch [21/25] | Batch [350/949] | D_loss: 0.1691 | G_loss: 3.3516\n",
            "Epoch [21/25] | Batch [400/949] | D_loss: 0.0724 | G_loss: 4.4592\n",
            "Epoch [21/25] | Batch [450/949] | D_loss: 0.0834 | G_loss: 5.2350\n",
            "Epoch [21/25] | Batch [500/949] | D_loss: 0.2209 | G_loss: 3.5352\n",
            "Epoch [21/25] | Batch [550/949] | D_loss: 0.2080 | G_loss: 3.0844\n",
            "Epoch [21/25] | Batch [600/949] | D_loss: 0.4767 | G_loss: 2.5930\n",
            "Epoch [21/25] | Batch [650/949] | D_loss: 0.0861 | G_loss: 3.7082\n",
            "Epoch [21/25] | Batch [700/949] | D_loss: 0.0410 | G_loss: 4.9864\n",
            "Epoch [21/25] | Batch [750/949] | D_loss: 0.0178 | G_loss: 5.9866\n",
            "Epoch [21/25] | Batch [800/949] | D_loss: 1.6318 | G_loss: 5.2363\n",
            "Epoch [21/25] | Batch [850/949] | D_loss: 0.5137 | G_loss: 2.4506\n",
            "Epoch [21/25] | Batch [900/949] | D_loss: 0.2610 | G_loss: 3.4583\n",
            "Epoch [22/25] | Batch [0/949] | D_loss: 1.2523 | G_loss: 2.6801\n",
            "Epoch [22/25] | Batch [50/949] | D_loss: 0.2011 | G_loss: 2.7864\n",
            "Epoch [22/25] | Batch [100/949] | D_loss: 0.2791 | G_loss: 4.3065\n",
            "Epoch [22/25] | Batch [150/949] | D_loss: 4.4743 | G_loss: 10.4124\n",
            "Epoch [22/25] | Batch [200/949] | D_loss: 0.2791 | G_loss: 3.1220\n",
            "Epoch [22/25] | Batch [250/949] | D_loss: 0.0385 | G_loss: 4.9475\n",
            "Epoch [22/25] | Batch [300/949] | D_loss: 0.6668 | G_loss: 2.0598\n",
            "Epoch [22/25] | Batch [350/949] | D_loss: 0.2559 | G_loss: 3.7335\n",
            "Epoch [22/25] | Batch [400/949] | D_loss: 0.1085 | G_loss: 3.7662\n",
            "Epoch [22/25] | Batch [450/949] | D_loss: 0.0437 | G_loss: 4.8393\n",
            "Epoch [22/25] | Batch [500/949] | D_loss: 0.0132 | G_loss: 6.0516\n",
            "Epoch [22/25] | Batch [550/949] | D_loss: 0.8055 | G_loss: 1.7896\n",
            "Epoch [22/25] | Batch [600/949] | D_loss: 0.4031 | G_loss: 2.5405\n",
            "Epoch [22/25] | Batch [650/949] | D_loss: 0.4651 | G_loss: 3.9121\n",
            "Epoch [22/25] | Batch [700/949] | D_loss: 0.0705 | G_loss: 3.8831\n",
            "Epoch [22/25] | Batch [750/949] | D_loss: 0.0109 | G_loss: 5.7532\n",
            "Epoch [22/25] | Batch [800/949] | D_loss: 0.0244 | G_loss: 5.0538\n",
            "Epoch [22/25] | Batch [850/949] | D_loss: 0.0103 | G_loss: 6.7867\n",
            "Epoch [22/25] | Batch [900/949] | D_loss: 0.0086 | G_loss: 6.4678\n",
            "Epoch [23/25] | Batch [0/949] | D_loss: 0.0251 | G_loss: 6.2365\n",
            "Epoch [23/25] | Batch [50/949] | D_loss: 0.0098 | G_loss: 4.9362\n",
            "Epoch [23/25] | Batch [100/949] | D_loss: 0.3236 | G_loss: 2.6877\n",
            "Epoch [23/25] | Batch [150/949] | D_loss: 0.1787 | G_loss: 3.8192\n",
            "Epoch [23/25] | Batch [200/949] | D_loss: 0.1252 | G_loss: 3.5583\n",
            "Epoch [23/25] | Batch [250/949] | D_loss: 0.0207 | G_loss: 5.2068\n",
            "Epoch [23/25] | Batch [300/949] | D_loss: 0.0126 | G_loss: 5.7892\n",
            "Epoch [23/25] | Batch [350/949] | D_loss: 0.0122 | G_loss: 5.8563\n",
            "Epoch [23/25] | Batch [400/949] | D_loss: 0.0082 | G_loss: 6.4517\n",
            "Epoch [23/25] | Batch [450/949] | D_loss: 0.0050 | G_loss: 7.4572\n",
            "Epoch [23/25] | Batch [500/949] | D_loss: 15.6264 | G_loss: 5.4495\n",
            "Epoch [23/25] | Batch [550/949] | D_loss: 0.1361 | G_loss: 3.5439\n",
            "Epoch [23/25] | Batch [600/949] | D_loss: 0.0821 | G_loss: 4.3859\n",
            "Epoch [23/25] | Batch [650/949] | D_loss: 0.0165 | G_loss: 5.3930\n",
            "Epoch [23/25] | Batch [700/949] | D_loss: 0.0083 | G_loss: 6.1484\n",
            "Epoch [23/25] | Batch [750/949] | D_loss: 0.0140 | G_loss: 6.4158\n",
            "Epoch [23/25] | Batch [800/949] | D_loss: 0.0143 | G_loss: 6.2825\n",
            "Epoch [23/25] | Batch [850/949] | D_loss: 0.0087 | G_loss: 6.4630\n",
            "Epoch [23/25] | Batch [900/949] | D_loss: 0.0083 | G_loss: 6.2799\n",
            "Epoch [24/25] | Batch [0/949] | D_loss: 0.0040 | G_loss: 6.8067\n",
            "Epoch [24/25] | Batch [50/949] | D_loss: 0.0073 | G_loss: 6.8931\n",
            "Epoch [24/25] | Batch [100/949] | D_loss: 0.0028 | G_loss: 7.0839\n",
            "Epoch [24/25] | Batch [150/949] | D_loss: 0.0037 | G_loss: 6.9313\n",
            "Epoch [24/25] | Batch [200/949] | D_loss: 0.0027 | G_loss: 7.2201\n",
            "Epoch [24/25] | Batch [250/949] | D_loss: 0.0021 | G_loss: 7.3016\n",
            "Epoch [24/25] | Batch [300/949] | D_loss: 0.0072 | G_loss: 7.4704\n",
            "Epoch [24/25] | Batch [350/949] | D_loss: 0.0054 | G_loss: 7.6278\n",
            "Epoch [24/25] | Batch [400/949] | D_loss: 0.8816 | G_loss: 1.5344\n",
            "Epoch [24/25] | Batch [450/949] | D_loss: 1.3642 | G_loss: 3.6297\n",
            "Epoch [24/25] | Batch [500/949] | D_loss: 0.4227 | G_loss: 2.6989\n",
            "Epoch [24/25] | Batch [550/949] | D_loss: 0.4097 | G_loss: 3.5435\n",
            "Epoch [24/25] | Batch [600/949] | D_loss: 0.5127 | G_loss: 2.0404\n",
            "Epoch [24/25] | Batch [650/949] | D_loss: 0.3376 | G_loss: 5.5683\n",
            "Epoch [24/25] | Batch [700/949] | D_loss: 0.1602 | G_loss: 3.3165\n",
            "Epoch [24/25] | Batch [750/949] | D_loss: 0.7598 | G_loss: 1.3137\n",
            "Epoch [24/25] | Batch [800/949] | D_loss: 0.1963 | G_loss: 3.3685\n",
            "Epoch [24/25] | Batch [850/949] | D_loss: 0.1913 | G_loss: 4.8810\n",
            "Epoch [24/25] | Batch [900/949] | D_loss: 0.0535 | G_loss: 4.3082\n",
            "Epoch [25/25] | Batch [0/949] | D_loss: 0.2045 | G_loss: 3.7143\n",
            "Epoch [25/25] | Batch [50/949] | D_loss: 0.0465 | G_loss: 4.7443\n",
            "Epoch [25/25] | Batch [100/949] | D_loss: 0.0250 | G_loss: 5.0503\n",
            "Epoch [25/25] | Batch [150/949] | D_loss: 0.0194 | G_loss: 5.9353\n",
            "Epoch [25/25] | Batch [200/949] | D_loss: 0.0214 | G_loss: 4.8593\n",
            "Epoch [25/25] | Batch [250/949] | D_loss: 0.0070 | G_loss: 5.4915\n",
            "Epoch [25/25] | Batch [300/949] | D_loss: 0.0443 | G_loss: 3.2620\n",
            "Epoch [25/25] | Batch [350/949] | D_loss: 0.0072 | G_loss: 8.0068\n",
            "Epoch [25/25] | Batch [400/949] | D_loss: 0.0070 | G_loss: 6.9572\n",
            "Epoch [25/25] | Batch [450/949] | D_loss: 0.0034 | G_loss: 7.5101\n",
            "Epoch [25/25] | Batch [500/949] | D_loss: 0.0076 | G_loss: 6.7491\n",
            "Epoch [25/25] | Batch [550/949] | D_loss: 0.0062 | G_loss: 6.5770\n",
            "Epoch [25/25] | Batch [600/949] | D_loss: 0.0033 | G_loss: 6.8115\n",
            "Epoch [25/25] | Batch [650/949] | D_loss: 0.0050 | G_loss: 6.8635\n",
            "Epoch [25/25] | Batch [700/949] | D_loss: 1.1552 | G_loss: 1.5033\n",
            "Epoch [25/25] | Batch [750/949] | D_loss: 0.4533 | G_loss: 2.1544\n",
            "Epoch [25/25] | Batch [800/949] | D_loss: 0.3065 | G_loss: 2.9462\n",
            "Epoch [25/25] | Batch [850/949] | D_loss: 0.2085 | G_loss: 3.9502\n",
            "Epoch [25/25] | Batch [900/949] | D_loss: 0.2458 | G_loss: 4.5968\n",
            "Saved models at epoch 25\n",
            "\n",
            "--- Training Finished ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec9cd763",
        "outputId": "0132db46-7765-49b9-ff1d-fb34996e734a"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "npy_file_path = \"/content/full_numpy_bitmap_camel.npy\"\n",
        "\n",
        "try:\n",
        "    # Load the data from the .npy file\n",
        "    data = np.load(npy_file_path)\n",
        "\n",
        "    print(f\"Successfully loaded the file: {npy_file_path}\")\n",
        "    print(f\"Data Type: {data.dtype}\")\n",
        "    print(f\"Shape: {data.shape}\")\n",
        "\n",
        "    # You can add more analysis here if needed, e.g., display the first few rows:\n",
        "    # print(\"\\nFirst 5 rows of the data:\")\n",
        "    # print(data[:5])\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file was not found at {npy_file_path}\")\n",
        "    print(\"Please make sure the file exists at the specified path.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading or analyzing the file: {e}\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded the file: /content/full_numpy_bitmap_camel.npy\n",
            "Data Type: uint8\n",
            "Shape: (121399, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xVKHkaHnLemZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}